# -*- coding: utf-8 -*-
"""QLora finetune

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UPor1Z7BjiSZoneF-GZ8Ao-G7--UpXp_
"""

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments, DataCollatorForLanguageModeling
import torch
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel, LoraModel

def create_model(huggingface_id, tokenizer_id=None, four_bit=True, use_cpu=False):
    if tokenizer_id is None:
        tokenizer_id = huggingface_id
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    if not four_bit:
        bnb_config = None
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)
    tokenizer.pad_token = tokenizer.eos_token

    device_map = {"":0}
    if use_cpu:
        device_map = {"": "cpu"}

    model = AutoModelForCausalLM.from_pretrained(huggingface_id, quantization_config=bnb_config, device_map=device_map,trust_remote_code=True)
    model.gradient_checkpointing_enable()
    return model, tokenizer

def load_lora(model, lora_path):
    model = PeftModel.from_pretrained(model, lora_path)
    return model

def loraify_model(model, rank=8, alpha=32, dropout=0.05):
    model = prepare_model_for_kbit_training(model)
    config = LoraConfig(
        r=rank,
        lora_alpha=alpha,
        target_modules=["q_proj", "v_proj"], # For LLaMA
        # target_modules=["query_key_value", "xxx"], # For RedPajama
        lora_dropout=dropout,
        bias="none",
        task_type="CAUSAL_LM"
    )
    model = get_peft_model(model, config)
    return model

def create_dataset(file_name, tokenizer):
    from datasets import load_dataset
    dataset = load_dataset("json", data_files={"train": file_name})
    def tokenize_function(examples):
        return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=256)
    dataset = dataset.map(tokenize_function, batched=True, remove_columns=["text"])
    dataset = dataset["train"]
    return dataset

def train_model(model, tokenizer, dataset, lora_name, batch_size=4, gradient_accum=4, max_steps=10000, save_steps=200, logging_steps=5, num_train_epochs=10):
    trainer = Trainer(
        model=model,
        train_dataset=dataset,
        args=TrainingArguments(
            per_device_train_batch_size=batch_size,
            gradient_accumulation_steps=gradient_accum,
            warmup_steps=2,
            learning_rate=2e-4,
            fp16=True,
            logging_steps=logging_steps,
            output_dir="outputs",
            optim="paged_adamw_8bit",
            save_steps=save_steps,
            num_train_epochs=num_train_epochs,
        ),
        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)
    )
    model.config.use_cache = False
    trainer.train()
    model.save_pretrained(lora_name)

def generate_completion(model, tokenizer, prompt, max_length):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=max_length)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# TRAINING QLORA
model, tokenizer = create_model("model_id")
model = loraify_model(model, rank=8, alpha=16, dropout=0.1)
dataset = create_dataset("data.json", tokenizer)
train_model(model, tokenizer, dataset, "lora_name")

# INFERENCE
model, tokenizer = create_model("model_id")
model = load_lora(model, "lora_name")
completion = generate_completion(model, tokenizer, "LoRA is", 100)
print(completion)